{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib qt\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")\n",
    "\n",
    "# Load the data from the Excel file\n",
    "df = pd.read_excel('Aggregated.xlsx')\n",
    "\n",
    "# Extract sequences and labels\n",
    "X = df['Sequence'].tolist()\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "# Preprocess sequences\n",
    "X_train_preprocessed = [' '.join(seq) for seq in X_train]\n",
    "X_test_preprocessed = [' '.join(seq) for seq in X_test]\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "# Tokenize and encode the data\n",
    "max_length = 60  # based on distribution of seq lengths\n",
    "train_encodings = tokenizer(X_train_preprocessed, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "valid_encodings = tokenizer(X_test_preprocessed, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "# Move the tokenized data to the selected device\n",
    "train_encodings = {key: val.to(device) for key, val in train_encodings.items()}\n",
    "valid_encodings = {key: val.to(device) for key, val in valid_encodings.items()}\n",
    "\n",
    "# Load the pre-trained model and move it to the device\n",
    "model = BertModel.from_pretrained('Rostlab/prot_bert_bfd_localization').to(device)\n",
    "\n",
    "def extract_embeddings_in_batches(encodings, model, batch_size=16):\n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_mask = encodings['attention_mask']\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_ids), batch_size):\n",
    "            batch_input_ids = input_ids[i:i+batch_size]\n",
    "            batch_attention_mask = attention_mask[i:i+batch_size]\n",
    "            \n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Extract embeddings for training and validation sets\n",
    "train_embeddings = extract_embeddings_in_batches(train_encodings, model)\n",
    "valid_embeddings = extract_embeddings_in_batches(valid_encodings, model)\n",
    "\n",
    "# Prepare the data for t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "train_tsne_embeddings = tsne.fit_transform(train_embeddings)\n",
    "valid_tsne_embeddings = tsne.fit_transform(valid_embeddings)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "train_df = pd.DataFrame(train_tsne_embeddings, columns=['x', 'y'])\n",
    "train_df['label'] = y_train\n",
    "train_df['set'] = 'train'\n",
    "\n",
    "valid_df = pd.DataFrame(valid_tsne_embeddings, columns=['x', 'y'])\n",
    "valid_df['label'] = y_test\n",
    "valid_df['set'] = 'test'\n",
    "\n",
    "df_tsne = pd.concat([train_df, valid_df])\n",
    "\n",
    "# Map labels to colors\n",
    "df_tsne['color'] = df_tsne.apply(lambda row: f\"{row['set']}_\" + ('AMPs' if row['label'] == 1 else 'non-AMPs'), axis=1)\n",
    "\n",
    "# Plotting the t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='color',\n",
    "    palette={\n",
    "        'train_AMPs': 'blue',\n",
    "        'train_non-AMPs': 'red',\n",
    "        'test_AMPs': 'green',\n",
    "        'test_non-AMPs': 'orange'\n",
    "    },\n",
    "    data=df_tsne,\n",
    "    legend='full',\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title('t-SNE of BERT Sequence Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "%matplotlib qt\n",
    "\n",
    "# Check if GPU is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU available, using CPU.\")\n",
    "\n",
    "# Load the data from the Excel file\n",
    "df = pd.read_excel('Aggregated.xlsx')\n",
    "\n",
    "# Extract sequences and labels\n",
    "X = df['Sequence'].tolist()\n",
    "y = df['Label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "test_size = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "# Preprocess sequences\n",
    "X_train_preprocessed = [' '.join(seq) for seq in X_train]\n",
    "X_test_preprocessed = [' '.join(seq) for seq in X_test]\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\")\n",
    "\n",
    "# Tokenize and encode the data\n",
    "max_length = 60  # based on distribution of seq lengths\n",
    "train_encodings = tokenizer(X_train_preprocessed, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "valid_encodings = tokenizer(X_test_preprocessed, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "# Move the tokenized data to the selected device\n",
    "train_encodings = {key: val.to(device) for key, val in train_encodings.items()}\n",
    "valid_encodings = {key: val.to(device) for key, val in valid_encodings.items()}\n",
    "\n",
    "# Define the classifier model with your custom weights\n",
    "class ProteinClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(ProteinClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('Rostlab/prot_bert_bfd_localization')\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.4),\n",
    "            nn.Linear(self.bert.config.hidden_size, n_classes),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return output.last_hidden_state\n",
    "\n",
    "# Load the pre-trained model architecture\n",
    "model = ProteinClassifier(n_classes=1).to(device)\n",
    "\n",
    "# Load your trained model weights\n",
    "model.load_state_dict(torch.load(\"protein_classifier_model.pth\", map_location=device))\n",
    "\n",
    "def extract_embeddings_in_batches(encodings, model, batch_size=16):\n",
    "    input_ids = encodings['input_ids']\n",
    "    attention_mask = encodings['attention_mask']\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_ids), batch_size):\n",
    "            batch_input_ids = input_ids[i:i+batch_size]\n",
    "            batch_attention_mask = attention_mask[i:i+batch_size]\n",
    "            \n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            embeddings = outputs.mean(dim=1).cpu().numpy()\n",
    "            all_embeddings.append(embeddings)\n",
    "    \n",
    "    return np.vstack(all_embeddings)\n",
    "\n",
    "# Extract embeddings for training and validation sets\n",
    "train_embeddings = extract_embeddings_in_batches(train_encodings, model)\n",
    "valid_embeddings = extract_embeddings_in_batches(valid_encodings, model)\n",
    "\n",
    "# Prepare the data for t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "train_tsne_embeddings = tsne.fit_transform(train_embeddings)\n",
    "valid_tsne_embeddings = tsne.fit_transform(valid_embeddings)\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "train_df = pd.DataFrame(train_tsne_embeddings, columns=['x', 'y'])\n",
    "train_df['label'] = y_train\n",
    "train_df['set'] = 'train'\n",
    "\n",
    "valid_df = pd.DataFrame(valid_tsne_embeddings, columns=['x', 'y'])\n",
    "valid_df['label'] = y_test\n",
    "valid_df['set'] = 'test'\n",
    "\n",
    "df_tsne = pd.concat([train_df, valid_df])\n",
    "\n",
    "# Map labels to colors\n",
    "df_tsne['color'] = df_tsne.apply(lambda row: f\"{row['set']}_\" + ('AMPs' if row['label'] == 1 else 'non-AMPs'), axis=1)\n",
    "\n",
    "# Plotting the t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(\n",
    "    x='x', y='y',\n",
    "    hue='color',\n",
    "    palette={\n",
    "        'train_AMPs': 'blue',\n",
    "        'train_non-AMPs': 'red',\n",
    "        'test_AMPs': 'green',\n",
    "        'test_non-AMPs': 'orange'\n",
    "    },\n",
    "    data=df_tsne,\n",
    "    legend='full',\n",
    "    alpha=0.6\n",
    ")\n",
    "plt.title('t-SNE of BERT Sequence Embeddings after Fine-Tuning')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
